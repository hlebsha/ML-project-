{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fbee551-72a0-4997-a786-07adfc95c7a0",
   "metadata": {},
   "source": [
    "**Loading images and labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d3b5189c-ff08-4fda-9226-1d19f692e5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def load_data(data_dir, augment=False):\n",
    "    images, labels = [], []\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "    for label in os.listdir(data_dir):\n",
    "        for image_file in os.listdir(os.path.join(data_dir, label)):\n",
    "            img = image.load_img(os.path.join(data_dir, label, image_file), target_size=(256, 256))\n",
    "            img_tensor = image.img_to_array(img)\n",
    "            img_tensor = np.expand_dims(img_tensor, axis=0)\n",
    "            img_tensor /= 255.  # normalize to [0,1] range\n",
    "\n",
    "            if augment:\n",
    "                aug_images = [img_tensor]\n",
    "                for batch in datagen.flow(img_tensor, batch_size=1):\n",
    "                    aug_images.append(batch)\n",
    "                    if len(aug_images) >= 6:  # original + 5 augmented images\n",
    "                        break\n",
    "                img_tensor = np.concatenate(aug_images, axis=0)\n",
    "\n",
    "            images.append(img_tensor)\n",
    "            labels.extend([label] * len(img_tensor))\n",
    "\n",
    "    return np.concatenate(images, axis=0), np.array(labels)\n",
    "\n",
    "images, labels = load_data('ds', augment=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbf0f71-9eda-499e-abbf-c2c1089fc28b",
   "metadata": {},
   "source": [
    "**DS split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d13d9bea-207b-4158-b68f-c85047bd0ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3081, 256, 256, 3)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f0c336-48f8-452c-92d1-41d318572dc2",
   "metadata": {},
   "source": [
    "**Model teaching**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "272c1388-8cf7-4d9d-b6a3-321737ebf3ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_11\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_11\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196608</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │   <span style=\"color: #00af00; text-decoration-color: #00af00\">201,327,616</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,040</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten_4 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196608\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_33 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │   \u001b[38;5;34m201,327,616\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_34 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m262,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_35 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m16,448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_36 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │         \u001b[38;5;34m1,040\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_37 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m51\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">201,607,555</span> (769.07 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m201,607,555\u001b[0m (769.07 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">201,607,555</span> (769.07 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m201,607,555\u001b[0m (769.07 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 2s/step - accuracy: 0.4372 - loss: 32.3211 - val_accuracy: 0.5447 - val_loss: 0.9253\n",
      "Epoch 2/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 2s/step - accuracy: 0.5660 - loss: 1.0204 - val_accuracy: 0.5447 - val_loss: 1.0724\n",
      "Epoch 3/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 1s/step - accuracy: 0.5663 - loss: 1.0616 - val_accuracy: 0.5447 - val_loss: 1.0482\n",
      "Epoch 4/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 1s/step - accuracy: 0.5663 - loss: 1.0349 - val_accuracy: 0.5447 - val_loss: 1.0287\n",
      "Epoch 5/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 1s/step - accuracy: 0.5663 - loss: 1.0133 - val_accuracy: 0.5447 - val_loss: 1.0136\n",
      "Epoch 6/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 1s/step - accuracy: 0.5663 - loss: 0.9960 - val_accuracy: 0.5447 - val_loss: 1.0023\n",
      "Epoch 7/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 1s/step - accuracy: 0.5663 - loss: 0.9827 - val_accuracy: 0.5447 - val_loss: 0.9940\n",
      "Epoch 8/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 1s/step - accuracy: 0.5663 - loss: 0.9724 - val_accuracy: 0.5447 - val_loss: 0.9881\n",
      "Epoch 9/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 1s/step - accuracy: 0.5663 - loss: 0.9646 - val_accuracy: 0.5447 - val_loss: 0.9840\n",
      "Epoch 10/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 1s/step - accuracy: 0.5663 - loss: 0.9588 - val_accuracy: 0.5447 - val_loss: 0.9812\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1c3f21790d0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(256, 256, 3)))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax')) \n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f0eaac-37df-4756-a0de-6c540d33af5f",
   "metadata": {},
   "source": [
    "**Predict and evaluate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ecf8ec86-8c22-491d-a402-c751ae3de112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step\n",
      "{0: 922, 1: 1770, 2: 389}\n",
      "{0: 236, 1: 420, 2: 115}\n",
      "Accuracy: 0.5447470817120622\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fist       0.00      0.00      0.00       236\n",
      "        Palm       0.54      1.00      0.71       420\n",
      "       Thumb       0.00      0.00      0.00       115\n",
      "\n",
      "    accuracy                           0.54       771\n",
      "   macro avg       0.18      0.33      0.24       771\n",
      "weighted avg       0.30      0.54      0.38       771\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\DevTools\\Python\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\DevTools\\Python\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\DevTools\\Python\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d6d552-f521-4e2f-aa68-d93cd3135b8f",
   "metadata": {},
   "source": [
    "**Magic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa0e7d4b-28fd-4d1a-8257-9ba9536ef2cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[43mrecognize_gesture\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msvm_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPalm\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     50\u001b[0m     cnt_palm \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[18], line 25\u001b[0m, in \u001b[0;36mrecognize_gesture\u001b[0;34m(frame, model, le)\u001b[0m\n\u001b[1;32m     23\u001b[0m gray_frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[1;32m     24\u001b[0m gray_frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(gray_frame, (\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m))\n\u001b[0;32m---> 25\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mhog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgray_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morientations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpixels_per_cell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcells_per_block\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mL2-Hys\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     26\u001b[0m prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(features)\n\u001b[1;32m     27\u001b[0m label \u001b[38;5;241m=\u001b[39m le\u001b[38;5;241m.\u001b[39minverse_transform(prediction)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/skimage/_shared/utils.py:316\u001b[0m, in \u001b[0;36mchannel_as_last_axis.__call__.<locals>.fixed_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m channel_axis \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannel_axis\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m channel_axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;66;03m# TODO: convert scalars to a tuple in anticipation of eventually\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;66;03m#       supporting a tuple of channel axes. Right now, only an\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m#       integer or a single-element tuple is supported, though.\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(channel_axis):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/skimage/feature/_hog.py:203\u001b[0m, in \u001b[0;36mhog\u001b[0;34m(image, orientations, pixels_per_cell, cells_per_block, block_norm, visualize, transform_sqrt, feature_vector, channel_axis)\u001b[0m\n\u001b[1;32m    201\u001b[0m     g_col \u001b[38;5;241m=\u001b[39m g_col_by_ch[rr, cc, idcs_max]\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     g_row, g_col \u001b[38;5;241m=\u001b[39m \u001b[43m_hog_channel_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03mThe third stage aims to produce an encoding that is sensitive to\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03mlocal image content while remaining resistant to small changes in\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03mcell are used to vote into the orientation histogram.\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m s_row, s_col \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/skimage/feature/_hog.py:43\u001b[0m, in \u001b[0;36m_hog_channel_gradient\u001b[0;34m(channel)\u001b[0m\n\u001b[1;32m     41\u001b[0m g_col[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     42\u001b[0m g_col[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 43\u001b[0m g_col[:, \u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mchannel\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchannel\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m g_row, g_col\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import time \n",
    "\n",
    "pygame.mixer.init()\n",
    "\n",
    "playlist = ['song1.mp3', 'song2.mp3']\n",
    "current_song = 0\n",
    "\n",
    "def play_music():\n",
    "    global current_song\n",
    "    pygame.mixer.music.load(playlist[current_song])\n",
    "    pygame.mixer.music.play()\n",
    "\n",
    "def stop_music():\n",
    "    pygame.mixer.music.stop()\n",
    "    \n",
    "def skip_music():\n",
    "    global current_song\n",
    "    current_song = (current_song + 1) % len(playlist)\n",
    "    play_music()\n",
    "\n",
    "def recognize_gesture(frame, model, le):\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    gray_frame = cv2.resize(gray_frame, (256, 256))\n",
    "    features = hog(gray_frame, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(4, 4), block_norm='L2-Hys').reshape(1, -1)\n",
    "    prediction = model.predict(features)\n",
    "    label = le.inverse_transform(prediction)\n",
    "    return label[0]\n",
    "\n",
    "cnt_fist = 0\n",
    "cnt_palm = 0\n",
    "cnt_thumb = 0\n",
    "\n",
    "def clear_cnt():\n",
    "    cnt_fist = 0\n",
    "    cnt_palm = 0\n",
    "    cnt_thumb = 0\n",
    "    \n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "play_music()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    label = recognize_gesture(frame, svm_model, le)\n",
    "    if label == 'Palm':\n",
    "        cnt_palm += 1\n",
    "        if cnt_palm > 10 :\n",
    "            stop_music()\n",
    "            clear_cnt()\n",
    "            \n",
    "    elif label == 'Thumb':\n",
    "        cnt_thumb += 1\n",
    "        if cnt_thumb > 10 :\n",
    "            skip_music()\n",
    "            clear_cnt()\n",
    "            \n",
    "    elif label == 'Fist':\n",
    "        cnt_fist += 1\n",
    "        if cnt_fist > 10 :\n",
    "            play_music()\n",
    "            clear_cnt()\n",
    "    \n",
    "    cv2.putText(frame, label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.imshow('Gesture Recognition', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "pygame.mixer.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0cba8d-2bae-4e36-9140-e0097ed4e611",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
