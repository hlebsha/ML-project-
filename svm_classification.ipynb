{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c44492c-4c04-4b54-86f3-fc450971a471",
   "metadata": {},
   "source": [
    "**Augementation UDFs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f27ade8a-2370-4dad-a482-c1255bec5938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def random_rotation(image):\n",
    "    angle = np.random.uniform(-30, 30) #rotate between -30 and 30 degrees\n",
    "    h, w = image.shape[:2]\n",
    "    M = cv2.getRotationMatrix2D((w / 2, h / 2), angle, 1)\n",
    "    rotated = cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_LINEAR)\n",
    "    return rotated\n",
    "\n",
    "def random_flip(image):\n",
    "    flip_code = 1  #horizontal flip\n",
    "    flipped = cv2.flip(image, flip_code)\n",
    "    return flipped\n",
    "\n",
    "def random_shift(image):\n",
    "    max_shift = 25  # shift up to 25 pixels\n",
    "    rows, cols = image.shape\n",
    "    M = np.float32([[1, 0, np.random.randint(-max_shift, max_shift)],\n",
    "                    [0, 1, np.random.randint(-max_shift, max_shift)]])\n",
    "    shifted = cv2.warpAffine(image, M, (cols, rows))\n",
    "    return shifted\n",
    "\n",
    "def random_zoom(image):\n",
    "    zoom_factor = np.random.uniform(1.0, 1.5)\n",
    "    height, width = image.shape[:2]\n",
    "    new_height, new_width = int(height * zoom_factor), int(width * zoom_factor)\n",
    "    \n",
    "    zoomed_image = cv2.resize(image, (new_width, new_height))\n",
    "    crop_height = (new_height - height) // 2\n",
    "    crop_width = (new_width - width) // 2\n",
    "    zoomed_image = zoomed_image[crop_height:crop_height+height, crop_width:crop_width+width]\n",
    "    return zoomed_image\n",
    "\n",
    "def random_brightness(image):\n",
    "    brightness_factor = np.random.uniform(0.5, 1.5)\n",
    "    br_image = cv2.convertScaleAbs(image, alpha=brightness_factor, beta=0)\n",
    "    return br_image\n",
    "\n",
    "def random_blur(image):\n",
    "    ksize = 3\n",
    "    blurred_image = cv2.GaussianBlur(image, (ksize, ksize), 0)\n",
    "    return blurred_image\n",
    "\n",
    "def augment_image(image):\n",
    "    if np.random.rand() < 0.5:\n",
    "        image = random_rotation(image)\n",
    "    if np.random.rand() < 0.5:\n",
    "        image = random_flip(image)\n",
    "    if np.random.rand() < 0.5:\n",
    "        image = random_shift(image)\n",
    "    if np.random.rand() < 0.5:\n",
    "        image = random_zoom(image)\n",
    "    if np.random.rand() < 0.5:\n",
    "        image = random_brightness(image)\n",
    "    if np.random.rand() < 0.5:\n",
    "        image = random_blur(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbee551-72a0-4997-a786-07adfc95c7a0",
   "metadata": {},
   "source": [
    "**Loading images and labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3b5189c-ff08-4fda-9226-1d19f692e5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_data(data_dir, augment=False):\n",
    "    images, labels = [], []\n",
    "    for label in os.listdir(data_dir):\n",
    "        for image_file in os.listdir(os.path.join(data_dir, label)):\n",
    "            image = cv2.imread(os.path.join(data_dir, label, image_file), cv2.IMREAD_GRAYSCALE)\n",
    "            image = cv2.resize(image, (256, 256))  # will use fixed size for images\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "            if augment:\n",
    "                for i in range(5):  # 5 augnmented per image?\n",
    "                    aug_image = augment_image(image)\n",
    "                    images.append(aug_image)\n",
    "                    labels.append(label)\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "images, labels = load_data('ds', augment=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7076419b-50db-443f-b816-0ee3255de6ab",
   "metadata": {},
   "source": [
    "**Extracting hog features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a9eb9d7-dd78-4fca-b4e0-77ea0973baa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "\n",
    "def extract_hog_features(images):\n",
    "    hog_features = []\n",
    "    for image in images:\n",
    "        features = hog(image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(4, 4), block_norm='L2-Hys')\n",
    "        hog_features.append(features)\n",
    "    return np.array(hog_features)\n",
    "\n",
    "hog_features = extract_hog_features(images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbf0f71-9eda-499e-abbf-c2c1089fc28b",
   "metadata": {},
   "source": [
    "**DS split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d13d9bea-207b-4158-b68f-c85047bd0ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(hog_features, labels, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f0c336-48f8-452c-92d1-41d318572dc2",
   "metadata": {},
   "source": [
    "**Model teaching**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "272c1388-8cf7-4d9d-b6a3-321737ebf3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 ... 1 0 1]\n",
      "[1 0 1 2 2 0 1 1 1 1 2 1 0 1 1 1 0 1 1 1 0 2 1 0 0 1 1 1 0 0 2 0 0 1 0 0 1\n",
      " 2 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 0 0 1 1 0 0 1 1 2 0 1 0 1 0 1 0 2\n",
      " 1 1 2 0 1 1 0 0 1 0 1 1 0 1 1 0 0 1 1 1 0 0 0 1 1 0 0 0 1 1 2 1 1 1 0 1 2\n",
      " 1 1 1 1 1 0 1 1 1 1 2 1 0 2 0 1 0 1 1 1 1 1 1 1 0 1 0 1 2 1 1 0 2 1 2 0 1\n",
      " 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 2 2 1 2 1 1 1 1 1 1 0 0 2 1 1 1 0 1 0 1 1\n",
      " 1 0 2 1 1 1 0 1 2 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 2 0 0 0 1 1 0 1 0 0 1 1\n",
      " 1 2 0 0 2 1 1 1 1 1 1 1 1 2 2 0 1 1 0 2 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0\n",
      " 1 0 0 0 0 0 1 1 0 1 1 2 1 1 2 1 1 0 1 1 1 1 1 2 1 0 1 1 0 2 2 0 0 0 0 0 0\n",
      " 1 2 1 2 2 0 1 0 2 0 0 0 2 1 2 1 1 2 2 2 1 0 0 0 1 0 0 0 1 2 1 1 1 1 2 0 1\n",
      " 1 2 2 0 0 2 1 1 1 2 0 1 1 1 0 0 1 0 2 1 1 2 1 0 1 0 2 2 1 1 1 1 1 2 0 1 1\n",
      " 1 1 0 0 1 1 1 0 0 0 1 1 2 1 2 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 2 0 1 1 1 1 0\n",
      " 0 2 2 1 1 1 1 2 0 1 0 2 0 0 1 1 1 1 0 1 1 2 0 0 1 0 1 1 1 1 0 1 0 2 1 2 2\n",
      " 2 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 1 2 1 1 0 1 1 1 0 1 0\n",
      " 0 1 0 2 1 0 0 2 1 0 0 1 1 1 1 1 1 1 1 0 1 2 2 1 1 0 2 0 1 2 1 0 1 0 1 1 0\n",
      " 0 0 0 1 1 1 2 1 0 1 1 0 2 1 1 1 0 1 0 0 1 2 1 1 1 1 1 1 1 1 1 0 0 1 0 2 2\n",
      " 1 1 2 0 1 1 1 1 1 1 0 1 1 1 2 1 1 1 0 0 1 0 0 1 0 2 1 2 1 0 1 1 1 2 1 1 2\n",
      " 1 1 0 2 1 0 1 0 1 0 1 0 2 0 1 1 0 0 0 1 0 1 1 1 1 1 1 2 0 1 1 0 1 0 1 1 1\n",
      " 1 1 2 1 0 0 0 1 1 0 0 2 1 2 0 1 1 1 2 1 1 2 1 2 1 1 0 1 1 1 0 1 0 1 1 1 0\n",
      " 1 1 1 2 0 1 0 0 1 2 1 1 1 1 1 1 1 1 0 1 1 1 1 1 2 1 0 0 1 0 1 1 0 0 1 1 1\n",
      " 0 0 0 0 1 0 1 0 1 1 1 0 1 1 1 1 2 1 0 1 0 0 0 1 2 1 1 1 0 1 1 0 1 0 0 1 0\n",
      " 1 0 2 1 0 1 1 1 1 0 1 2 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 2 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "# Train SVM model\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train, y_train_encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f0eaac-37df-4756-a0de-6c540d33af5f",
   "metadata": {},
   "source": [
    "**Predict and evaluate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecf8ec86-8c22-491d-a402-c751ae3de112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.933852140077821\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fist       0.91      0.92      0.92       245\n",
      "        Palm       0.95      0.96      0.95       426\n",
      "       Thumb       0.93      0.87      0.90       100\n",
      "\n",
      "    accuracy                           0.93       771\n",
      "   macro avg       0.93      0.92      0.92       771\n",
      "weighted avg       0.93      0.93      0.93       771\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = svm_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test_encoded, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(classification_report(y_test_encoded, y_pred, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d6d552-f521-4e2f-aa68-d93cd3135b8f",
   "metadata": {},
   "source": [
    "**Magic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa0e7d4b-28fd-4d1a-8257-9ba9536ef2cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[43mrecognize_gesture\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msvm_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPalm\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     50\u001b[0m     cnt_palm \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[18], line 25\u001b[0m, in \u001b[0;36mrecognize_gesture\u001b[0;34m(frame, model, le)\u001b[0m\n\u001b[1;32m     23\u001b[0m gray_frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[1;32m     24\u001b[0m gray_frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(gray_frame, (\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m))\n\u001b[0;32m---> 25\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mhog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgray_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morientations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpixels_per_cell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcells_per_block\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mL2-Hys\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     26\u001b[0m prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(features)\n\u001b[1;32m     27\u001b[0m label \u001b[38;5;241m=\u001b[39m le\u001b[38;5;241m.\u001b[39minverse_transform(prediction)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/skimage/_shared/utils.py:316\u001b[0m, in \u001b[0;36mchannel_as_last_axis.__call__.<locals>.fixed_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m channel_axis \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannel_axis\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m channel_axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;66;03m# TODO: convert scalars to a tuple in anticipation of eventually\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;66;03m#       supporting a tuple of channel axes. Right now, only an\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m#       integer or a single-element tuple is supported, though.\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(channel_axis):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/skimage/feature/_hog.py:203\u001b[0m, in \u001b[0;36mhog\u001b[0;34m(image, orientations, pixels_per_cell, cells_per_block, block_norm, visualize, transform_sqrt, feature_vector, channel_axis)\u001b[0m\n\u001b[1;32m    201\u001b[0m     g_col \u001b[38;5;241m=\u001b[39m g_col_by_ch[rr, cc, idcs_max]\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     g_row, g_col \u001b[38;5;241m=\u001b[39m \u001b[43m_hog_channel_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03mThe third stage aims to produce an encoding that is sensitive to\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03mlocal image content while remaining resistant to small changes in\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03mcell are used to vote into the orientation histogram.\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m s_row, s_col \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/skimage/feature/_hog.py:43\u001b[0m, in \u001b[0;36m_hog_channel_gradient\u001b[0;34m(channel)\u001b[0m\n\u001b[1;32m     41\u001b[0m g_col[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     42\u001b[0m g_col[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 43\u001b[0m g_col[:, \u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mchannel\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchannel\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m g_row, g_col\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import time \n",
    "\n",
    "pygame.mixer.init()\n",
    "\n",
    "playlist = ['song1.mp3', 'song2.mp3']\n",
    "current_song = 0\n",
    "\n",
    "def play_music():\n",
    "    global current_song\n",
    "    pygame.mixer.music.load(playlist[current_song])\n",
    "    pygame.mixer.music.play()\n",
    "\n",
    "def stop_music():\n",
    "    pygame.mixer.music.stop()\n",
    "    \n",
    "def skip_music():\n",
    "    global current_song\n",
    "    current_song = (current_song + 1) % len(playlist)\n",
    "    play_music()\n",
    "\n",
    "def recognize_gesture(frame, model, le):\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    gray_frame = cv2.resize(gray_frame, (256, 256))\n",
    "    features = hog(gray_frame, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(4, 4), block_norm='L2-Hys').reshape(1, -1)\n",
    "    prediction = model.predict(features)\n",
    "    label = le.inverse_transform(prediction)\n",
    "    return label[0]\n",
    "\n",
    "cnt_fist = 0\n",
    "cnt_palm = 0\n",
    "cnt_thumb = 0\n",
    "\n",
    "def clear_cnt():\n",
    "    cnt_fist = 0\n",
    "    cnt_palm = 0\n",
    "    cnt_thumb = 0\n",
    "    \n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "play_music()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    label = recognize_gesture(frame, svm_model, le)\n",
    "    if label == 'Palm':\n",
    "        cnt_palm += 1\n",
    "        if cnt_palm > 10 :\n",
    "            stop_music()\n",
    "            clear_cnt()\n",
    "            \n",
    "    elif label == 'Thumb':\n",
    "        cnt_thumb += 1\n",
    "        if cnt_thumb > 10 :\n",
    "            skip_music()\n",
    "            clear_cnt()\n",
    "            \n",
    "    elif label == 'Fist':\n",
    "        cnt_fist += 1\n",
    "        if cnt_fist > 10 :\n",
    "            play_music()\n",
    "            clear_cnt()\n",
    "    \n",
    "    cv2.putText(frame, label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.imshow('Gesture Recognition', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "pygame.mixer.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0cba8d-2bae-4e36-9140-e0097ed4e611",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
